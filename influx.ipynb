{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "3209b2a512676eec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T19:00:58.872904Z",
     "start_time": "2024-09-03T19:00:57.445568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import logger_setup\n",
    "\n",
    "import pandas as pd\n",
    "from influxdb_client import InfluxDBClient\n",
    "# Ciekawy statek:\n",
    "# 215131000\n",
    "\n",
    "import os\n",
    "from influxdb_client import Point\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "\n",
    "from csv_reader import ais_csv_to_df"
   ],
   "id": "31942b8046633a01",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Functions",
   "id": "a577f9b3cd047730"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T19:00:59.891591Z",
     "start_time": "2024-09-03T19:00:59.874413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_point(row: pd.Series, measurement_name: str,\n",
    "                 mmsi_fieldname=\"MMSI\", vessel_name_fieldname=\"VesselName\",\n",
    "                 latitude_fieldname=\"LAT\", longitude_fieldname=\"LON\", time_fieldname=\"BaseDateTime\"\n",
    "                 ):\n",
    "    t = \"vessels_ais_31_12\"\n",
    "    point = (\n",
    "        Point(measurement_name=measurement_name)\n",
    "        .tag(\"mmsi\", row[mmsi_fieldname])\n",
    "        .tag(\"vessel_name\", row[vessel_name_fieldname])\n",
    "        .field(\"lat\", row[latitude_fieldname])\n",
    "        .field(\"lon\", row[longitude_fieldname])\n",
    "        .time(row[time_fieldname])\n",
    "    )\n",
    "    return point\n",
    "\n",
    "\n",
    "def upload_df_to_influx_in_batches(df: pd.DataFrame, influx_client: InfluxDBClient, bucket_name: str,\n",
    "                                   organization_id: str,\n",
    "                                   batch_size: int = 100000,\n",
    "                                   data_frame_tag_columns=[\"MMSI\", \"VesselName\", \"CallSign\", \"VesselType\", \"Status\",\n",
    "                                                           \"Length\", \"Width\", \"Cargo\", \"TransceiverClass\"]):\n",
    "    logger.debug(f\"Uploading to influxdb. Batch size: {batch_size}.\")\n",
    "    write_api = influx_client.write_api(write_options=SYNCHRONOUS)\n",
    "\n",
    "    rows = df.shape[0]\n",
    "    divisions = rows // batch_size + 1\n",
    "    dfs = np.array_split(df, divisions)\n",
    "\n",
    "    for i in range(divisions):\n",
    "        logger.debug(f\"Uploading division {i}/{divisions - 1}. Shape: {dfs[i].shape}. Processing...\")\n",
    "        write_api.write(bucket=bucket_name, org=organization_id,\n",
    "                        record=dfs[i],\n",
    "                        data_frame_measurement_name=\"vessels_ais_31_12\",\n",
    "                        data_frame_tag_columns=data_frame_tag_columns,\n",
    "                        data_frame_timestamp_column=\"BaseDateTime\",\n",
    "                        )"
   ],
   "id": "f0aa6db85522d236",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup",
   "id": "af124525f3186806"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T19:14:00.372275Z",
     "start_time": "2024-09-03T19:14:00.365192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logger = logger_setup.setup_logging()\n",
    "load_dotenv()\n",
    "# token = os.environ.get(\"API_INFLUX_KEY_10\")\n",
    "# org = os.environ.get(\"INFLUX_ORG_ID_10\")\n",
    "# url = \"http://localhost:\" + os.environ.get(\"INFLUX_PORT\", \"55001\")\n",
    "token = os.environ.get(\"API_INFLUX_KEY_CLOUD\")\n",
    "org = os.environ.get(\"INFLUX_CLOUD_ORG\")\n",
    "url = os.environ.get(\"INFLUX_CLOUD_HOST\")\n",
    "\n",
    "logger.debug(f\"Token: {token}\")\n",
    "logger.debug(f\"Organization id: {org}\")\n",
    "logger.info(f\"Database endpoint: {url}\")"
   ],
   "id": "f42e4e2856e36d8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-03 21:14:00,369 - DEBUG: Token: Z4xDybyzy6j6pv92BvOwWB6fmQLFBLPMcGGJb8pEaC0PLcv3TY5KrBpTO6MXJhbj6lPMWHZ1rDEjUrB0iGdD1A==\n",
      "2024-09-03 21:14:00,370 - DEBUG: Organization id: Geoinformatyka\n",
      "2024-09-03 21:14:00,370 - INFO: Database endpoint: https://us-east-1-1.aws.cloud2.influxdata.com\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Setting spatial index\n",
    "\n",
    "Influx uses s2 cells for this purpose.\n",
    "\n",
    "In query this is `geo.shapeData()` function that does it. \n",
    "\n",
    "`s2_cell_id` has to be **saved as a tag** for other functions (such as `geo.filterRows()`) to work. "
   ],
   "id": "d6c6657a42c20148"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T17:06:24.535570Z",
     "start_time": "2024-08-11T16:31:03.870129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "\n",
    "client = InfluxDBClient(url=url, token=token, org=org, timeout=60000)  # Set timeout to 60 seconds\n",
    "query_api = client.query_api()\n",
    "\n",
    "\n",
    "def generate_time_ranges_for_day(date: datetime.date, interval_minutes: float):\n",
    "    current_start = datetime.datetime.combine(date, datetime.time.min)\n",
    "    end_of_day = datetime.datetime.combine(date, datetime.time.max)\n",
    "    while current_start < end_of_day:\n",
    "        current_end = current_start + datetime.timedelta(minutes=interval_minutes)\n",
    "        if current_end > end_of_day:\n",
    "            current_end = end_of_day\n",
    "        yield current_start.isoformat() + 'Z', current_end.isoformat() + 'Z'\n",
    "        current_start = current_end\n",
    "\n",
    "\n",
    "day = datetime.date(2020, 12, 31)\n",
    "interval = 30\n",
    "\n",
    "# Are these correct?\n",
    "raw_data_bucket = \"temp_bucket_2\"\n",
    "# indexed_data_bucket = \"shapedData_bucket2\"\n",
    "# indexed_data_bucket = \"shapedData_bucket3\"\n",
    "indexed_data_bucket = \"aisdata_s2indexed_lvl20\"\n",
    "\n",
    "lat_field_name = \"LAT\"\n",
    "lon_field_name = \"LON\"\n",
    "level = 20\n",
    "\n",
    "for start, end in generate_time_ranges_for_day(day, interval):\n",
    "    print(f\"From: {start}, to: {end}\", end=\" \")\n",
    "    flux_query = f\"\"\"\n",
    "import \"experimental/geo\"\n",
    "\n",
    "from(bucket: \"{raw_data_bucket}\")\n",
    "    |> range(start: {start}, stop: {end})\n",
    "    |> filter(fn: (r) => r._measurement == \"vessels_ais_31_12\")\n",
    "    |> filter(fn: (r) => r._field == \"LAT\" or r._field == \"LON\")\n",
    "    |> geo.shapeData(latField: \"{lat_field_name}\", lonField: \"{lon_field_name}\", level: {level})\n",
    "    |> to\n",
    "        (bucket: \"{indexed_data_bucket}\", tagColumns: [\"s2_cell_id\", \"MMSI\"], fieldFn: (r) => ({{\"lat\": r.lat, \"lon\": r.lon}}))\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query\n",
    "    result = query_api.query(flux_query)\n",
    "    print(f\"Level {level}, Finished.\")\n",
    "    \n"
   ],
   "id": "800f30474c3aec21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: 2020-12-31T00:00:00Z, to: 2020-12-31T00:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T00:30:00Z, to: 2020-12-31T01:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T01:00:00Z, to: 2020-12-31T01:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T01:30:00Z, to: 2020-12-31T02:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T02:00:00Z, to: 2020-12-31T02:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T02:30:00Z, to: 2020-12-31T03:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T03:00:00Z, to: 2020-12-31T03:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T03:30:00Z, to: 2020-12-31T04:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T04:00:00Z, to: 2020-12-31T04:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T04:30:00Z, to: 2020-12-31T05:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T05:00:00Z, to: 2020-12-31T05:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T05:30:00Z, to: 2020-12-31T06:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T06:00:00Z, to: 2020-12-31T06:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T06:30:00Z, to: 2020-12-31T07:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T07:00:00Z, to: 2020-12-31T07:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T07:30:00Z, to: 2020-12-31T08:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T08:00:00Z, to: 2020-12-31T08:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T08:30:00Z, to: 2020-12-31T09:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T09:00:00Z, to: 2020-12-31T09:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T09:30:00Z, to: 2020-12-31T10:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T10:00:00Z, to: 2020-12-31T10:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T10:30:00Z, to: 2020-12-31T11:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T11:00:00Z, to: 2020-12-31T11:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T11:30:00Z, to: 2020-12-31T12:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T12:00:00Z, to: 2020-12-31T12:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T12:30:00Z, to: 2020-12-31T13:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T13:00:00Z, to: 2020-12-31T13:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T13:30:00Z, to: 2020-12-31T14:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T14:00:00Z, to: 2020-12-31T14:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T14:30:00Z, to: 2020-12-31T15:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T15:00:00Z, to: 2020-12-31T15:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T15:30:00Z, to: 2020-12-31T16:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T16:00:00Z, to: 2020-12-31T16:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T16:30:00Z, to: 2020-12-31T17:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T17:00:00Z, to: 2020-12-31T17:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T17:30:00Z, to: 2020-12-31T18:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T18:00:00Z, to: 2020-12-31T18:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T18:30:00Z, to: 2020-12-31T19:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T19:00:00Z, to: 2020-12-31T19:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T19:30:00Z, to: 2020-12-31T20:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T20:00:00Z, to: 2020-12-31T20:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T20:30:00Z, to: 2020-12-31T21:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T21:00:00Z, to: 2020-12-31T21:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T21:30:00Z, to: 2020-12-31T22:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T22:00:00Z, to: 2020-12-31T22:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T22:30:00Z, to: 2020-12-31T23:00:00Z Level 20, Finished.\n",
      "From: 2020-12-31T23:00:00Z, to: 2020-12-31T23:30:00Z Level 20, Finished.\n",
      "From: 2020-12-31T23:30:00Z, to: 2020-12-31T23:59:59.999999Z Level 20, Finished.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Spatial query and spatiotemporal query\n",
    "\n",
    "Well to be honest it's a spatiotemporal query, because influx requires you to specify time range for every query. This still takes full time into consideration."
   ],
   "id": "bcaf8cf869b24d07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T19:27:57.883870Z",
     "start_time": "2024-09-03T19:27:57.878247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Bbox:\n",
    "    def __init__(self, min_lon, min_lat, max_lon, max_lat, id):\n",
    "        self.min_lon = min_lon\n",
    "        self.min_lat = min_lat\n",
    "        self.max_lon = max_lon\n",
    "        self.max_lat = max_lat\n",
    "        self.id = id\n",
    "\n",
    "    def get_coords(self):\n",
    "        return [self.min_lon, self.min_lat, self.max_lon, self.max_lat]\n",
    "    \n",
    "    def get_id(self):\n",
    "        return self.id\n",
    "    \n",
    "bounding_boxes = [\n",
    "    Bbox(-76, 34, -75.9, 34.3, 0), \n",
    "    Bbox(-123.247925, 48.136125, -122.739476, 48.362910, 0), \n",
    "]"
   ],
   "id": "f9e8686946a12f7",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T19:41:32.788829Z",
     "start_time": "2024-09-03T19:41:32.152867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = InfluxDBClient(url=url, token=token, org=org, timeout=60000)  # Set timeout to 60 seconds\n",
    "query_api = client.query_api()\n",
    "\n",
    "# bucket = \"shapedData_bucket2\"\n",
    "# bucket = \"temp\"\n",
    "# bucket = \"aisdata_s2indexed_lvl24\"\n",
    "start_date = \"2020-12-31T00:00:00Z\"\n",
    "stop_date = \"2020-12-31T00:00:59Z\"\n",
    "\n",
    "bucket = \"aisdata\"\n",
    "start_date = \"2024-09-01T00:00:00Z\"\n",
    "stop_date = \"2024-09-01T00:00:59Z\"\n",
    "\n",
    "min_lon, min_lat, max_lon, max_lat = bounding_boxes[0].get_coords()\n",
    "level = 10\n",
    "strict = \"true\"\n",
    "start_time = time.time()\n",
    "MMSI = \"636017540\"\n",
    "query = f\"\"\"\n",
    "import \"experimental/geo\"\n",
    "\n",
    "region = {{\n",
    "    minLat: {min_lat},\n",
    "    maxLat: {max_lat},\n",
    "    minLon: {min_lon},  \n",
    "    maxLon: {max_lon},\n",
    "}}\n",
    "\n",
    "from(bucket: \"{bucket}\")\n",
    "    |> range(start: {start_date}, stop: {stop_date})\n",
    "    |> filter(fn: (r) => r._measurement == \"vessels_ais_31_12\")\n",
    "    |> filter(fn: (r) => r._field == \"LAT\" or r._field == \"LON\")\n",
    "    |> filter(fn: (r) => r.MMSI == \"{MMSI}\")\n",
    "    |> geo.shapeData(latField: \"LAT\", lonField: \"LON\", level: {level})\n",
    "    // |> geo.filterRows(region: region, level: {level}, strict: true)\n",
    "\"\"\"\n",
    "# //   |> geo.shapeData(latField: \"LAT\", lonField: \"LON\", level: 24)\n",
    "# //   |> drop(columns: [\"_measurement\", \"Status\", \"TransceiverClass\", \"VesselName\", \"VesselType\", \"CallSign\"])\n",
    "# //   |> geo.filterRows(region: region, level: 24, strict: true)\n",
    "tables = query_api.query(query)\n",
    "\n",
    "end_time = time.time()\n",
    "record_count = sum(len(table.records) for table in tables)\n",
    "logger.info(f\"Query took {end_time - start_time} seconds, no. of results: {record_count}\")\n",
    "\n",
    "logger.info(\"Closing database connection...\")\n",
    "client.close()"
   ],
   "id": "d46961e78f1632b4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-03 21:41:32,785 - INFO: Query took 0.6288387775421143 seconds, no. of results: 1\n",
      "2024-09-03 21:41:32,787 - INFO: Closing database connection...\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T19:41:34.089727Z",
     "start_time": "2024-09-03T19:41:34.087019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for table in tables:\n",
    "    for record in table.records:\n",
    "        print(record)"
   ],
   "id": "f24e0fdf1002ef23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FluxRecord() table: 0, {'result': '_result', 'table': 0, 'MMSI': '636017540', 'Status': '0.0', 'TransceiverClass': 'A', '_measurement': 'vessels_ais_31_12', '_start': datetime.datetime(2024, 9, 1, 0, 0, tzinfo=tzutc()), '_stop': datetime.datetime(2024, 9, 1, 0, 0, 59, tzinfo=tzutc()), '_time': datetime.datetime(2024, 9, 1, 0, 0, 1, tzinfo=tzutc()), 'lat': 34.25975, 'lon': -75.98961, 's2_cell_id': '89a7b1'}\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T19:21:27.992560Z",
     "start_time": "2024-09-03T19:21:27.758962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import folium\n",
    "def get_results_folium(influx_tables=[],  bounding_boxes=[]):\n",
    "    CHICAGO_COORDINATES = (42, -95)\n",
    "\n",
    "    map_attributions = ('&copy; <a href=\"http://www.openstreetmap.org/copyright\">OpenStreetMap</a> '\n",
    "                        'contributors, &copy; <a href=\"http://cartodb.com/attributions\">CartoDB</a>')\n",
    "\n",
    "    m = folium.Map(location=CHICAGO_COORDINATES,\n",
    "                   attr=map_attributions,\n",
    "                   zoom_start=5,\n",
    "                   control_scale=True,\n",
    "                   height=800,\n",
    "                   width=1400)\n",
    "    \n",
    "    for table in influx_tables:\n",
    "        for record in table:\n",
    "            popup = folium.Popup(f\"mmsi: {record['MMSI']}, time: {record['_time']}\")\n",
    "            folium.Marker(location=[record[\"lat\"], record[\"lon\"]],\n",
    "                          popup=popup,\n",
    "                          icon=folium.Icon(color='blue', icon='ship', prefix='fa')).add_to(m)\n",
    "\n",
    "    for bbox in bounding_boxes:\n",
    "        coords = bbox.get_coords()\n",
    "        min_lon, min_lat, max_lon, max_lat = coords\n",
    "\n",
    "        # Create the bounding box as a rectangle\n",
    "        folium.Rectangle(\n",
    "            bounds=[(min_lat, min_lon), (max_lat, max_lon)],\n",
    "            color=\"blue\",  # You can change the color as needed\n",
    "            fill=True,\n",
    "            popup=bbox.get_id(),\n",
    "            fill_opacity=0.2\n",
    "        ).add_to(m)\n",
    "\n",
    "    return m\n",
    "    \n",
    "m: folium.Map = get_results_folium(tables, bounding_boxes[0:1])\n",
    "m"
   ],
   "id": "d248a707f54793e5",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 48\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_lon, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_lat, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_lon, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_lat]\n\u001B[1;32m     45\u001B[0m bounding_boxes \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     46\u001B[0m     Bbox(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m123.247925\u001B[39m, \u001B[38;5;241m48.136125\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m122.739476\u001B[39m, \u001B[38;5;241m48.362910\u001B[39m), \n\u001B[1;32m     47\u001B[0m ]\n\u001B[0;32m---> 48\u001B[0m m: folium\u001B[38;5;241m.\u001B[39mMap \u001B[38;5;241m=\u001B[39m get_influx_results_folium(\u001B[43mtables\u001B[49m, bounding_boxes[\u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m     49\u001B[0m m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tables' is not defined"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Time query",
   "id": "8fe1ccf8061b8018"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T19:14:43.413135Z",
     "start_time": "2024-09-03T19:14:43.410933Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f100b816107ac97c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Uploading data - slow",
   "id": "8d00a13ae61e31fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T06:30:30.974667Z",
     "start_time": "2024-07-08T06:30:26.165573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Uploading data one by one - slow\n",
    "\n",
    "# safety check - don't run unless you really want to\n",
    "exit()\n",
    "\n",
    "client: InfluxDBClient = InfluxDBClient(url=url, token=token, org=org)\n",
    "bucket = \"temp_bucket_2\"\n",
    "\n",
    "df = ais_csv_to_df(\"data/AIS_2020_12_31.csv\")\n",
    "df[\"VesselName\"] = df[\"VesselName\"].str.replace(\" \", \"\\ \")\n",
    "print(\"Creating points...\")\n",
    "df[\"Points\"] = df.apply(create_point, axis=1, args=(\"vessels_ais_31_12\",))\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "start_time = time.time()\n",
    "print(\"Uploading points...\")\n",
    "for i, point in enumerate(df[\"Points\"]):\n",
    "    if i % 1000 == 0 and i != 0:\n",
    "        print(\n",
    "            f\"Point {i}: {point}. Time elapsed: {time.time() - start_time}. Average time per point: {(time.time() - start_time) / i}\")\n",
    "    write_api.write(bucket=bucket, org=org, record=point)\n",
    "\n",
    "logger.info(\"Closing database connection...\")\n",
    "client.close()"
   ],
   "id": "115d27a9b1e0e6d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 08:30:26,167 - DEBUG: Loading data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m client: InfluxDBClient \u001B[38;5;241m=\u001B[39m InfluxDBClient(url\u001B[38;5;241m=\u001B[39murl, token\u001B[38;5;241m=\u001B[39mtoken, org\u001B[38;5;241m=\u001B[39morg)\n\u001B[1;32m      4\u001B[0m bucket \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemp_bucket_2\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 6\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mais_csv_to_df\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata/AIS_2020_12_31.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVesselName\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVesselName\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating points...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/gits/spatiotemportal-databases-comparison/csv_reader.py:45\u001B[0m, in \u001B[0;36mais_csv_to_df\u001B[0;34m(csv_filename, timestamp_field_name)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mais_csv_to_df\u001B[39m(csv_filename: \u001B[38;5;28mstr\u001B[39m, timestamp_field_name: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBaseDateTime\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame:\n\u001B[1;32m     44\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading data...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 45\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcsv_filename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparse_dates\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mtimestamp_field_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/gits/spatiotemportal-databases-comparison/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/gits/spatiotemportal-databases-comparison/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[0;32m--> 626\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/gits/spatiotemportal-databases-comparison/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1923\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m   1916\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[1;32m   1917\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1918\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[1;32m   1919\u001B[0m     (\n\u001B[1;32m   1920\u001B[0m         index,\n\u001B[1;32m   1921\u001B[0m         columns,\n\u001B[1;32m   1922\u001B[0m         col_dict,\n\u001B[0;32m-> 1923\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[1;32m   1924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[1;32m   1925\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1926\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m   1927\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/Documents/gits/spatiotemportal-databases-comparison/venv/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[0;32m--> 234\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    235\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[1;32m    236\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[0;32mparsers.pyx:838\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mparsers.pyx:921\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mparsers.pyx:1083\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mparsers.pyx:1456\u001B[0m, in \u001B[0;36mpandas._libs.parsers._maybe_upcast\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Documents/gits/spatiotemportal-databases-comparison/venv/lib/python3.10/site-packages/numpy/core/multiarray.py:1131\u001B[0m, in \u001B[0;36mputmask\u001B[0;34m(a, mask, values)\u001B[0m\n\u001B[1;32m   1082\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1083\u001B[0m \u001B[38;5;124;03m    copyto(dst, src, casting='same_kind', where=True)\u001B[39;00m\n\u001B[1;32m   1084\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \n\u001B[1;32m   1127\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (dst, src, where)\n\u001B[0;32m-> 1131\u001B[0m \u001B[38;5;129m@array_function_from_c_func_and_dispatcher\u001B[39m(_multiarray_umath\u001B[38;5;241m.\u001B[39mputmask)\n\u001B[1;32m   1132\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mputmask\u001B[39m(a, \u001B[38;5;241m/\u001B[39m, mask, values):\n\u001B[1;32m   1133\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1134\u001B[0m \u001B[38;5;124;03m    putmask(a, mask, values)\u001B[39;00m\n\u001B[1;32m   1135\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1171\u001B[0m \n\u001B[1;32m   1172\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   1173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (a, mask, values)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Uploading data - fast",
   "id": "28c37501cc0a2765"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T19:18:20.465569Z",
     "start_time": "2024-09-03T19:18:19.359322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Uploading data do influx database in batches - fast\n",
    "\n",
    "# safety check - don't run unless you really want to\n",
    "# exit()\n",
    "\n",
    "client: InfluxDBClient = InfluxDBClient(url=url, token=token, org=org)\n",
    "bucket = \"aisdata\"\n",
    "\n",
    "df = ais_csv_to_df(\"data/AIS_2020_12_31_first_2_sec.csv\")\n",
    "# df = df[[\"MMSI\", \"VesselName\", \"LAT\", \"LON\", \"BaseDateTime\"]]\n",
    "df[\"VesselName\"] = df[\"VesselName\"].str.replace(\" \", \"_\")\n",
    "df[\"CallSign\"] = df[\"CallSign\"].str.replace(\" \", \"_\")\n",
    "logger.debug(f\"Dataframe shape: {df.shape}\")\n",
    "\n",
    "logger.debug(\"Beware! Executing The Command!\")\n",
    "start_time = time.time()\n",
    "upload_df_to_influx_in_batches(df, client, bucket, org, 200000)\n",
    "end_time = time.time()\n",
    "logger.info(f\"Upload time: {end_time - start_time}\")\n",
    "\n",
    "logger.info(\"Closing database connection...\")\n",
    "client.close()"
   ],
   "id": "7945e7c63ba24070",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-03 21:18:19,362 - DEBUG: Loading data...\n",
      "2024-09-03 21:18:19,370 - DEBUG: Dataframe shape: (302, 17)\n",
      "2024-09-03 21:18:19,371 - DEBUG: Beware! Executing The Command!\n",
      "2024-09-03 21:18:19,371 - DEBUG: Uploading to influxdb. Batch size: 200000.\n",
      "/home/tymon/Documents/gits/spatiotemportal-databases-comparison/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "2024-09-03 21:18:19,373 - DEBUG: Uploading division 0/0. Shape: (302, 17). Processing...\n",
      "2024-09-03 21:18:20,462 - INFO: Upload time: 1.090437650680542\n",
      "2024-09-03 21:18:20,463 - INFO: Closing database connection...\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## How to connect to the database",
   "id": "fab0cbf1ca23460d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T06:29:04.517626Z",
     "start_time": "2024-07-08T06:29:04.514439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client: InfluxDBClient = InfluxDBClient(url=url, token=token, org=org)\n",
    "bucket = \"temp_bucket_2\"\n",
    "\n",
    "logger.info(\"Closing database connection...\")\n",
    "client.close()"
   ],
   "id": "1d5b0f053de0e3cf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 08:29:04,515 - INFO: Closing database connection...\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
